# üè∑Ô∏è BERT POS Tagger

This repository contains a **Part-of-Speech (POS) Tagger** using **BERT-based models** for token classification. It supports **training, prediction, and evaluation** of POS tagging using a pre-trained **Indic-BERT model**.

---

## üöÄ Features

‚úÖ **Pretrained Indic-BERT Model** ‚Äì Uses `ai4bharat/indic-bert` for accurate token classification.  
‚úÖ **POS Encoding Dictionary** ‚Äì Maps POS tags to numerical labels using `pos_encoding.pickle`.  
‚úÖ **Train & Fine-Tune** ‚Äì Train the model using **Hugging Face's Trainer API**.  
‚úÖ **Prediction & Inference** ‚Äì Convert input text into **POS-tagged** sentences.  
‚úÖ **Handles Ambiguities** ‚Äì Ensures token alignment and label mapping for robust tagging.  
‚úÖ **Supports Dataset Splitting** ‚Äì Uses train, validation, and test splits from a dataset.  

---

## üì• Installation & Setup

### 1Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/YathishPoojary98/BERT-POSTAGGER.git
```
Navigate into the cloned directory:
```bash
cd BERT-POSTAGGER
```

### 2Ô∏è‚É£ Install Dependencies
Install the required Python packages:
```bash
pip install transformers datasets numpy torch pickle5
```

---

## üèãÔ∏è Training the Model

### **Train the Model with `prepare_splits_train.py`**
To start training:
```bash
python prepare_splits_train.py
```
This script will:
- Load the dataset from pickle file
- Encode POS tags using `pos_encoding.pickle`
- Tokenize and align labels
- Train the model using Hugging Face's Trainer API
- Save the fine-tuned model to `output_dir`

---

## üéØ POS Tagging Inference

### **Run Predictions with `run_pos.py`**
To use the trained model for POS tagging:
```bash
python run_pos.py --input input.txt --output output.txt --model output_dir
```
#### Arguments:
- `--input` ‚Üí Path to input text file
- `--output` ‚Üí Path to save POS-tagged output
- `--model` ‚Üí Path to the trained model directory

This script will:
- Tokenize input sentences
- Predict POS tags using the fine-tuned model
- Write tagged sentences to `output.txt`

---

## üìÇ Repository Structure

```
BERT-POSTAGGER/
‚îÇ‚îÄ‚îÄ prepare_splits_train.py      # Training script
‚îÇ‚îÄ‚îÄ run_pos.py                   # POS tagging script
‚îÇ‚îÄ‚îÄ BERT POS.ipynb               # Jupyter Notebook for preprocessing
‚îÇ‚îÄ‚îÄ new_pos_train_data.pickle    # POS tag training data
‚îÇ‚îÄ‚îÄ new_pos_encoding.pickle      # POS tag encoding dictionary
‚îÇ‚îÄ‚îÄ output_dir/                  # Saved fine-tuned model
```

---

## üìä Example POS-Tagged Output
**Example input (`input.txt`)**:
```
‡≤Ö‡≤¶‡≤∞‡≤Ç‡≤§‡≥Ü
‡≤∂‡≤æ‡≤≤‡≥Ü‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø
‡≤π‡≥à‡≤ü‡≥Ü‡≤ï‡≥ç
‡≤∂‡≥å‡≤ö‡≤æ‡≤≤‡≤Ø‡≤ó‡≤≥
‡≤®‡≤ø‡≤∞‡≥ç‡≤Æ‡≤æ‡≤£‡≤¶‡≤ø‡≤Ç‡≤¶
'
‡≤∏‡≥ç‡≤µ‡≤ö‡≥ç‡≤õ‡≤§‡≥Ü‡≤Ø‡≥á
‡≤∏‡≥á‡≤µ‡≥Ü
'
‡≤ò‡≥ã‡≤∑‡≤£‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å
‡≤∏‡≤æ‡≤¨‡≥Ä‡≤§‡≥Å‡≤™‡≤°‡≤ø‡≤∏‡≤ø‡≤¶‡≥Ü
.

‡≤∂‡≤æ‡≤≤‡≥Ü‡≤ó‡≤≥
‡≤Ü‡≤µ‡≤∞‡≤£‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø
‡≤Ü‡≤ü‡≤¶
‡≤Æ‡≥à‡≤¶‡≤æ‡≤®‡≤ó‡≤≥
‡≤â‡≤®‡≥ç‡≤®‡≤§‡≥Ä‡≤ï‡≤∞‡≤£‡≤¶
‡≤Æ‡≥Ç‡≤≤‡≤ï
‡≤ï‡≥ç‡≤∞‡≥Ä‡≤°‡≤æ
‡≤ö‡≤ü‡≥Å‡≤µ‡≤ü‡≤ø‡≤ï‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü
‡≤â‡≤§‡≥ç‡≤§‡≥á‡≤ú‡≤®
‡≤®‡≥Ä‡≤°‡≤≤‡≥Å
‡≤Ü‡≤ó‡≤ø‡≤¶‡≥Ü
.
```
**Generated output (`output.txt`)**:
```
<Sentence id='1'>
1	‡≤Ö‡≤¶‡≤∞‡≤Ç‡≤§‡≥Ü	CC__CCS
2	‡≤∂‡≤æ‡≤≤‡≥Ü‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø	N__NN
3	‡≤π‡≥à‡≤ü‡≥Ü‡≤ï‡≥ç	JJ
4	‡≤∂‡≥å‡≤ö‡≤æ‡≤≤‡≤Ø‡≤ó‡≤≥	N__NN
5	‡≤®‡≤ø‡≤∞‡≥ç‡≤Æ‡≤æ‡≤£‡≤¶‡≤ø‡≤Ç‡≤¶	N__NN
6	'	RD__PUNC
7	‡≤∏‡≥ç‡≤µ‡≤ö‡≥ç‡≤õ‡≤§‡≥Ü‡≤Ø‡≥á	N__NN
8	‡≤∏‡≥á‡≤µ‡≥Ü	N__NN
9	'	RD__PUNC
10	‡≤ò‡≥ã‡≤∑‡≤£‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å	N__NNV
11	‡≤∏‡≤æ‡≤¨‡≥Ä‡≤§‡≥Å‡≤™‡≤°‡≤ø‡≤∏‡≤ø‡≤¶‡≥Ü	V__VM__VF
12	.	RD__PUNC
</Sentence>

<Sentence id='2'>
1	‡≤∂‡≤æ‡≤≤‡≥Ü‡≤ó‡≤≥	N__NN
2	‡≤Ü‡≤µ‡≤∞‡≤£‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø	N__NN
3	‡≤Ü‡≤ü‡≤¶	N__NNV
4	‡≤Æ‡≥à‡≤¶‡≤æ‡≤®‡≤ó‡≤≥	N__NN
5	‡≤â‡≤®‡≥ç‡≤®‡≤§‡≥Ä‡≤ï‡≤∞‡≤£‡≤¶	N__NN
6	‡≤Æ‡≥Ç‡≤≤‡≤ï	RP__RPD
7	‡≤ï‡≥ç‡≤∞‡≥Ä‡≤°‡≤æ	N__NN
8	‡≤ö‡≤ü‡≥Å‡≤µ‡≤ü‡≤ø‡≤ï‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü	N__NN
9	‡≤â‡≤§‡≥ç‡≤§‡≥á‡≤ú‡≤®	N__NN
10	‡≤®‡≥Ä‡≤°‡≤≤‡≥Å	V__VM__VF
11	‡≤Ü‡≤ó‡≤ø‡≤¶‡≥Ü	V__VAUX
12	.	RD__PUNC
</Sentence>
```

---
